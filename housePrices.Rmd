---
title: "House Price Prediction"
author: "Carlos Silva"
date: "`r format(Sys.Date())`"
output: 
   html_document:
     toc: true
     number_section: true
     theme: readable
     highlight: tango
     
---  

# Introduction

## Description

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences 
price negotiations than the number of bedrooms or a white-picket fence.

## Objective

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames and Iowa. the goal is to predict the sales price for each house. For each Id in the test set, the model will predict the value of the SalePrice variable. 

# Set up

## Load Packages

```{r setup, include=T, message = F, warning=F}
require(tidyverse)
require(skimr)
require(caret)
require(caretEnsemble)
require(xgboost)
require(kernlab)
require(Matrix)
require(recipes)
```

## Load Datasets

```{r, message = F, warning=F}
db1 <- read.csv('train.csv')
db2 <- read.csv('test.csv')
db2$SalePrice <- as.integer(NA)
hp <- rbind(db1, db2)
remove(db1); remove(db2)
```

# Overview of the Data

```{r}
skim(hp)
```

# Data Preparation

## Fix variable name starts with number

```{r}
names(hp)[44:45] <- c("FirstFLSF","SecFLSF")
names(hp)[70] <- c("ThreeSnPorch")
```

## Fix typos in factorial levels

```{r, warning=F}
count(hp, RoofMatl)
hp <- hp %>%
        mutate(RoofMatl = fct_collapse(RoofMatl, "TarGrv" = "Tar&Grv"))
```

```{r, warning=F}
count(hp, Exterior1st)
hp <- hp %>%
        mutate(Exterior1st = fct_collapse(Exterior1st, "WdSdng" = "Wd Sdng"))
```

```{r, warning=F}
count(hp, Exterior2nd)
hp <- hp %>%
        mutate(Exterior2nd = fct_collapse(Exterior2nd, "BrkComm" = "Brk Cmn", "WdSdng" = "Wd Sdng", "WdShng" = "Wd Shng"))
```

## Remove Outliers

we can identify two outlier points (Id = 524, 1299) in this plot SalePrice against GrLivArea, the other outlier points (Id = -463,-633,-1325,-31,-971) were identified after visualize the [residuals](#residuals) plot of the model fitted.

```{r, warning=F}
ggplot(hp, aes(x=GrLivArea, y=SalePrice)) +
        geom_point() +
        geom_smooth(method=lm, se=FALSE) +
        geom_text(data = hp[hp$GrLivArea>4500,], mapping=aes(label=Id), vjust=1.5, col = "blue") +
        xlab("above grade living area") +
        ylab("sale price")

outlier <- c(524, 1299, 463, 633, 1325, 31, 971)
hp<- hp[!hp$Id %in% outlier, ]
```

# Feature engineering

Transforming and combining variables to data set.

```{r}
hp <- hp  %>%
        mutate(withBsmt = as.numeric(TotalBsmtSF!=0),
               with2ndFloor= as.numeric(SecFLSF>0),
               withPool = as.numeric(PoolArea!=0),
               withPorch = as.numeric((OpenPorchSF+EnclosedPorch+ThreeSnPorch+ScreenPorch)!=0),
               hasRemod = as.numeric(YearRemodAdd != YearBuilt),
               withFireplace = as.numeric(Fireplaces >0),
               isNew = as.numeric(YrSold == YearBuilt),
               totalSF = GrLivArea + TotalBsmtSF,
               totalBath = FullBath + 0.5*HalfBath + 0.5*BsmtHalfBath + BsmtFullBath,
               age = YrSold - YearRemodAdd,
               yearsRemodeled = ifelse(YearRemodAdd == YearBuilt, YrSold < YearRemodAdd, 0),
               YrSold = factor(YrSold),
               MoSold = factor(MoSold),
               MSSubClass = factor(MSSubClass),
               OverallQual = factor(OverallQual),
               OverallCond = factor(OverallCond),
               totalPorchSF = OpenPorchSF + EnclosedPorch + ThreeSnPorch + ScreenPorch,
               withbsmtBath = as.numeric(BsmtHalfBath + BsmtFullBath!=0),
               isBsmtUnf = as.numeric(TotalBsmtSF == BsmtUnfSF)
        )
```

# Data preprocessing 

Recipe package was used to do preprocessing

## Split the data

```{r}
hp_train <- hp[!is.na(hp$SalePrice),]
hp_test <- hp[is.na(hp$SalePrice),]
set.seed(2123)
```

## Define the recipe

```{r, warning=F}
model_recipe <- recipe(SalePrice ~ ., data = hp_train) %>% 
        update_role(Id, new_role = "id variable") %>%
        step_knnimpute(all_predictors()) %>%
        step_dummy(all_predictors(), -all_numeric()) %>%
        step_BoxCox(all_predictors()) %>%
        step_center(all_predictors())  %>%
        step_scale(all_predictors()) %>%
        step_zv(all_predictors()) %>%
        step_corr(all_predictors(), threshold = .9) %>%
        step_log(all_outcomes()) %>%
        check_missing(all_predictors())

model_recipe
```

## Preparing and Bake the recipe

```{r, warning=F}
prepped_recipe <- prep(model_recipe, training = hp_train)
prepped_recipe

train <- bake(prepped_recipe, new_data = hp_train)
test <- bake(prepped_recipe, new_data = hp_test)
```

## Check the data 

```{r}
anyNA(subset(train, select=-SalePrice))

qqnorm(train$SalePrice)
qqline(train$SalePrice)
```

# Modeling

Use xgboost model to determin the best nRound value (number of iterations)

```{r}
params<-list(
        max_depth = 4, # default=6
        eta = 0.02, # it lies between 0.01 - 0.3
        gamma = 0, # default=0
        colsample_bytree = 0.65, # Typically, its values lie between (0.5,0.9)
        subsample = 0.6, # Typically, its values lie between (0.5-0.8)
        min_child_weight = 3 # default=1
)
```

## Preparing matrix 

```{r}
dtrain <- xgb.DMatrix(data = sparse.model.matrix(SalePrice~ .-Id, train), label= train$SalePrice)

```

## Calculate the best nround for this model

```{r}
set.seed(2123)
xgbcv <- xgb.cv( params = params, data = dtrain, label = train$SalePrice, nrounds = 2000, nfold = 10, showsd = F, stratified = T, print_every_n = 250, early_stopping_rounds = 50, maximize = F)

niter <- xgbcv$best_iteration
```

## Check the most important features

```{r}
xgb1 <- xgb.train(data = dtrain, params=params, nrounds = niter)
mat <- xgb.importance(feature_names = xgb1$feature_names, model = xgb1)
ggplot(mat[1:40,])+
        geom_bar(aes(x=reorder(Feature, Gain), y=Gain), stat='identity', fill='red')+
        xlab(label = "Features")+
        coord_flip() +
        ggtitle("Feature Importance")
```

## Do caret ensemble model

To make ensemble model were choosed a boosted tree model (xgboost), a regularized linear model (lasso), and a non-linear kernel SVM (svmRadial).

```{r, eval=FALSE, message=FALSE, warning=FALSE}
set.seed(2123)
trControl <- trainControl(
        method='cv',
        savePredictions="final",
        index = createFolds(train$SalePrice,k = 10, returnTrain = TRUE),
        allowParallel =TRUE, 
        verboseIter = TRUE
)

xgbGrid <- expand.grid(nrounds = niter, max_depth = c(3,4,5), eta = 0.02, gamma = 0, colsample_bytree = c(0.65),  subsample = c(0.6), min_child_weight = c(3,4,5))
glmGrid <- expand.grid(alpha = 1, lambda = seq(0.00001,0.01,by = 0.0001))
svmGrid <- expand.grid(sigma= 2^seq(-11, -16, -0.5), C= 2^seq(4,9,1))

modelList <<- caretList(
        x = subset(train, select=-c(Id, SalePrice)),
        y = train$SalePrice,
        trControl=trControl,
        metric="RMSE",
        tuneList=list(
                xgbTree = caretModelSpec(method="xgbTree",  tuneGrid = xgbGrid),
                glmnet=caretModelSpec(method="glmnet", tuneGrid = glmGrid),
                svmRadial = caretModelSpec(method="svmRadial", tuneGrid = svmGrid, preProcess=c("nzv", "pca"))
        )
)
```

# Model performance

## Check the tuning result, performance of each model and correlation between models

```{r}
plot(modelList$xgbTree)
plot(modelList$glmnet)
plot(modelList$svmRadial)
bwplot(resamples(modelList),metric="RMSE")
modelCor(resamples(modelList))
```

## Final stacking on the three models

```{r}
model_Ensemble <- caretEnsemble(   
        modelList, 
        metric="RMSE",
        trControl=trainControl(number=10, method = "repeatedcv", repeats=3)
)
summary(model_Ensemble)
```

## Check the residuals
<div id="residuals">
Visualize which data points gave highest prediction errors in the training set. The outliers were spotted at this step

```{r, warning=F}
pred.train <- predict(model_Ensemble, newdata=subset(train, select=-c(Id, SalePrice)))
hp.plot <- hp %>%
        filter(!is.na(SalePrice))
hp.plot$pred <- pred.train
hp.plot <- hp.plot %>%
        mutate(residual = log(SalePrice)-pred) 



ggplot(hp.plot, aes(x = pred, y = residual)) + 
        geom_pointrange(aes(ymin = 0, ymax = residual)) + 
        geom_hline(yintercept = 0, linetype = 3) + 
        geom_text(data = hp.plot[abs(hp.plot$residual) > 0.4,], aes(label = Id), vjust=1.5, col = "red") +
        ggtitle("Residuals vs. model prediction") +
        xlab("prediction") +
        ylab("residual") +
        theme(text = element_text(size=9))
```

## Final prediction

```{r}
finalPredictions <- predict(model_Ensemble, newdata=subset(test, select=-c(Id, SalePrice)))
finalPredictions <- data.frame('Id'= hp_test$Id, 'SalePrice'=exp(finalPredictions)) 
write.csv(finalPredictions, file="finalpredictions.csv", row.names = F)
```

