library(gapminder)
install.packages(gapminder)
install.packages("gapminder")
library(gapminder)
version
library(dplyr)
library(gapminder)
library(dplyr)
library(ggplot2)
gapminder_1952 <- gapminder %>%
filter(year == 1952)
gapminder
ggplot(gapminder_1952, aes( x = pop, y = gdpPercap, color = continent)
geom_point()
scale_x_log10("pop")
scale_y_log10("gdpPercap")
library(ggplot2)
gapminder_1952 <- gapminder %>%
filter(year == 1952)
ggplot(gapminder_1952, aes( x = pop, y = gdpPercap, color = continent) +
geom_point() +
scale_x_log10("pop") +
scale_y_log10("gdpPercap")
ggplot(gapminder_1952, aes( x = pop, y = gdpPercap, color = continent)) +
geom_point() +
scale_x_log10("pop") +
scale_y_log10("gdpPercap")
ggplot(gapminder_1952, aes( x = pop, y = gdpPercap, color = continent, size = pop)) +
geom_point() +
scale_x_log10("pop") +
scale_y_log10("gdpPercap")
update.packages()
update.packages()
update.packages()
update.packages()
update.packages()
update.packages(ask = FALSE)
update.packages(ask = FALSE)
update.packages(ask = FALSE)
install.packages(boot)
install.packages("boot")
update.packages(ask = FALSE)
install.packages("boot")
library(caret
)
library(data.table)
library(boot)
.libpaths()
libpaths()
.libpaths()
library(dplyr)
update.packages(ask = FALSE)
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
IRkernel::installspec()
IRkernel::installspec()
library(tidyverse)
library(caret)
?paralell
?parallel
?registerDoParallel
?preProcess
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("pandoc")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("Rtools")
update()
updateR()
library(installr)
updateR()
library("tidyverse")
library("caret")
library("caretEnsemble")
library("kernlab")
library("Matrix")
library("skimr")
library("recipes")
library("xgboost")
setwd("C:/Users/Utilizador/Desktop/kaggle/House_Prices-Advanced_Regression_Techniques")
set.seed(4561)
# Load data
db1 <- read.csv('train.csv')
db2 <- read.csv('test.csv')
db2$SalePrice <- as.integer(NA)
hp <- rbind(db1, db2)
remove(db1); remove(db2)
names(hp)[44:45] <- c("FirstFLSF","SecFLSF")
names(hp)[70] <- c("ThreeSnPorch")
# Fix typos in factorial levels
library(forcats)
count(hp, RoofMatl)
hp <- hp %>%
mutate(RoofMatl = fct_collapse(RoofMatl, "TarGrv" = "Tar&Grv"))
count(hp, Exterior1st)
hp <- hp %>%
mutate(Exterior1st = fct_collapse(Exterior1st, "WdSdng" = "Wd Sdng"))
count(hp, Exterior2nd)
hp <- hp %>%
mutate(Exterior2nd = fct_collapse(Exterior2nd, "BrkComm" = "Brk Cmn", "WdSdng" = "Wd Sdng", "WdShng" = "Wd Shng"))
# Fix typo of GarageYrBlt
summary(hp$GarageYrBlt)
hp <- hp %>%
mutate(GarageYrBlt = ifelse(GarageYrBlt > 2010, 2007, GarageYrBlt))
# remove Outliers
ggplot(hp, aes(x=GrLivArea, y=SalePrice)) +
geom_point() +
geom_smooth(method=lm, se=FALSE) +
geom_text(data=hp[hp$GrLivArea>4500,], mapping=aes(label=Id), vjust=1.5, col = "blue") +
xlab("above grade living area") +
ylab("sale price")
outlier <- c(524, 1299, 463, 633, 1325, 31, 971)
hp<- hp[!hp$Id %in% outlier, ]
hp <- hp  %>%
mutate(withBsmt = as.numeric(TotalBsmtSF!=0),
with2ndFloor= as.numeric(SecFLSF>0),
withPool = as.numeric(PoolArea!=0),
withPorch = as.numeric((OpenPorchSF+EnclosedPorch+ThreeSnPorch+ScreenPorch)!=0),
hasRemod = as.numeric(YearRemodAdd != YearBuilt),
withFireplace = as.numeric(Fireplaces >0),
isNew = as.numeric(YrSold == YearBuilt),
totalSF = GrLivArea + TotalBsmtSF,
totalBath = FullBath + 0.5*HalfBath + 0.5*BsmtHalfBath + BsmtFullBath,
age = YrSold - YearRemodAdd,
yearsRemodeled = ifelse(YearRemodAdd == YearBuilt, YrSold < YearRemodAdd, 0),
YrSold = factor(YrSold),
MoSold = factor(MoSold),
MSSubClass = factor(MSSubClass),
OverallQual = factor(OverallQual),
OverallCond = factor(OverallCond),
totalPorchSF = OpenPorchSF + EnclosedPorch + ThreeSnPorch + ScreenPorch,
withbsmtBath = as.numeric(BsmtHalfBath + BsmtFullBath!=0),
isBsmtUnf = as.numeric(TotalBsmtSF == BsmtUnfSF)
)
hp_train <- hp[!is.na(hp$SalePrice),]
hp_test <- hp[is.na(hp$SalePrice),]
set.seed(2123)
model_recipe <- recipe(SalePrice ~ ., data = hp_train) %>%
update_role(Id, new_role = "id var") %>%
step_knnimpute(all_predictors()) %>%
step_dummy(all_predictors(), -all_numeric()) %>%
step_BoxCox(all_predictors()) %>%
step_center(all_predictors())  %>%
step_scale(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_corr(all_predictors(), threshold = .9) %>%
step_log(all_outcomes()) %>%
check_missing(all_predictors())
model_recipe
prepped_recipe <- prep(model_recipe, training = hp_train)
prepped_recipe
train <- bake(prepped_recipe, new_data = hp_train)
test <- bake(prepped_recipe, new_data = hp_test)
anyNA(subset(train, select=-SalePrice))
qqnorm(train$SalePrice)
qqline(train$SalePrice)
skim(train)
params<-list(
max_depth = 4, # default=6
eta = 0.02, # it lies between 0.01 - 0.3
gamma = 0, # default=0
colsample_bytree = 0.65, # Typically, its values lie between (0.5,0.9)
subsample = 0.6, # Typically, its values lie between (0.5-0.8)
min_child_weight = 3 # default=1
)
dtrain <- xgb.DMatrix(data = sparse.model.matrix(SalePrice~ .-Id, train), label= train$SalePrice)
set.seed(2123)
xgbcv <- xgb.cv( params = params, data = dtrain, label = train$SalePrice, nrounds = 2000, nfold = 10, showsd = F, stratified = T, print_every_n = 250, early_stopping_rounds = 50, maximize = F)
niter <- xgbcv$best_iteration
niter
xgb1 <- xgb.train(data = dtrain, params=params, nrounds = niter)
mat <- xgb.importance(feature_names = xgb1$feature_names, model = xgb1)
ggplot(mat[1:40,])+
geom_bar(aes(x=reorder(Feature, Gain), y=Gain), stat='identity', fill='red')+
xlab(label = "Features")+
coord_flip() +
ggtitle("Feature Importance")
trControl <- trainControl(
method='cv',
savePredictions="final",
index = createFolds(train$SalePrice,k = 10, returnTrain = TRUE),
allowParallel =TRUE,
verboseIter = TRUE
)
xgbGrid <- expand.grid(nrounds = niter, max_depth = c(3,4,5), eta = 0.02, gamma = 0, colsample_bytree = c(0.65),  subsample = c(0.6), min_child_weight = c(3,4,5))
glmGrid <- expand.grid(alpha = 1, lambda = seq(0.00001,0.01,by = 0.0001))
svmGrid <- expand.grid(sigma= 2^seq(-11, -16, -0.5), C= 2^seq(4,9,1))
model <<- caretList(
x = subset(train, select=-c(Id, SalePrice)),
y = train$SalePrice,
trControl=trControl,
metric="RMSE",
tuneList=list(
xgb2 = caretModelSpec(method="xgbTree",  tuneGrid = xgbGrid),
glm=caretModelSpec(method="glmnet", tuneGrid = glmGrid),
svm = caretModelSpec(method="svmRadial", tuneGrid = svmGrid, preProcess=c("nzv", "pca"))
)
)
plot(model$xgb2)
plot(model$glm)
plot(model$svm)
bwplot(resamples(model),metric="RMSE")
modelCor(resamples(model))
model_Ensemble <- caretEnsemble(
model,
metric="RMSE",
trControl=trainControl(number=10, method = "repeatedcv", repeats=3)
)
summary(model_Ensemble)
model_Ensemble <- caretEnsemble(
model,
metric="RMSE",
trControl=trainControl(number=10, method = "repeatedcv", repeats=3)
)
summary(model_Ensemble)
pred.train <- predict(model_Ensemble, newdata=subset(train, select=-c(Id, SalePrice)))
hp.plot <- hp %>%
filter(!is.na(SalePrice))
hp.plot$pred <- pred.train
hp.plot <- hp.plot %>%
mutate(residual = log(SalePrice)-pred)
ggplot(hp.plot, aes(x = pred, y = residual)) +
geom_pointrange(aes(ymin = 0, ymax = residual)) +
geom_hline(yintercept = 0, linetype = 3) +
geom_text(data = hp.plot[abs(residual) > 0.4,], aes(label = Id), vjust=1.5, col = "red") +
ggtitle("Residuals vs. model prediction") +
xlab("prediction") +
ylab("residual") +
theme(text = element_text(size=9))
ggplot(hp.plot, aes(x = pred, y = residual)) +
geom_pointrange(aes(ymin = 0, ymax = residual)) +
geom_hline(yintercept = 0, linetype = 3) +
geom_text(data = hp.plot[abs(hp.plot$residual) > 0.4,], aes(label = Id), vjust=1.5, col = "red") +
ggtitle("Residuals vs. model prediction") +
xlab("prediction") +
ylab("residual") +
theme(text = element_text(size=9))
pred <- predict(model_Ensemble, newdata=subset(test, select=-c(Id, SalePrice)))
result <- data.frame('Id'= hp_test$Id, 'SalePrice'=exp(pred))
write.csv(result, file="final pred.csv", row.names = F)
model_recipe
model_recipe %>% class()
finalPredictions <- predict(model_Ensemble, newdata=subset(test, select=-c(Id, SalePrice)))
finalPredictions <- data.frame('Id'= hp_test$Id, 'SalePrice'=exp(finalPredictions))
write.csv(finalPredictions, file="final pred.csv", row.names = F)
write.csv(finalPredictions, file="finalpredictions.csv", row.names = F)
pred.train <- predict(model_Ensemble, newdata=subset(train, select=-c(Id, SalePrice)))
hp.plot <- hp %>%
filter(!is.na(SalePrice))
hp.plot$pred <- pred.train
hp.plot <- hp.plot %>%
mutate(residual = log(SalePrice)-pred)
ggplot(hp.plot, aes(x = pred, y = residual)) +
geom_pointrange(aes(ymin = 0, ymax = residual)) +
geom_hline(yintercept = 0, linetype = 3) +
geom_text(data = hp.plot[abs(hp.plot$residual) > 0.4,], aes(label = Id), vjust=1.5, col = "red") +
ggtitle("Residuals vs. model prediction") +
xlab("prediction") +
ylab("residual") +
theme(text = element_text(size=9))
?update_role
model_recipe <- recipe(SalePrice ~ ., data = hp_train) %>%
update_role(Id, new_role = "id var") %>%
step_knnimpute(all_predictors()) %>%
step_dummy(all_predictors(), -all_numeric()) %>%
step_BoxCox(all_predictors()) %>%
step_center(all_predictors())  %>%
step_scale(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_corr(all_predictors(), threshold = .9) %>%
step_log(all_outcomes()) %>%
check_missing(all_predictors())
model_recipe
install.packages(recipes)
prepped_recipe <- prep(model_recipe, training = hp_train)
prepped_recipe
plot(model$xgb2)
model <<- caretList(
x = subset(train, select=-c(Id, SalePrice)),
y = train$SalePrice,
trControl=trControl,
metric="RMSE",
tuneList=list(
xgb2 = caretModelSpec(method="xgbTree",  tuneGrid = xgbGrid),
glm=caretModelSpec(method="glmnet", tuneGrid = glmGrid),
svm = caretModelSpec(method="svmRadial", tuneGrid = svmGrid, preProcess=c("nzv", "pca"))
)
)
plot(model$svm)
?caretList
library(caretEnsemble)
library(caretEnsemble)
R.version
